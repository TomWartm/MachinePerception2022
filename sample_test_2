Sender: LSF System <lsfadmin@eu-g3-018>
Subject: Job 221582893: <python scripts/main.py --cfg configs/baseline.yaml> in cluster <euler> Exited

Job <python scripts/main.py --cfg configs/baseline.yaml> was submitted from host <eu-login-30> by user <aheser> in cluster <euler> at Tue Jun 14 13:00:16 2022
Job was executed on host(s) <6*eu-g3-018>, in queue <gpu.4h>, as user <aheser> in cluster <euler> at Tue Jun 14 13:00:32 2022
</cluster/home/aheser> was used as the home directory.
</cluster/home/aheser/project1_skeleton> was used as the working directory.
Started at Tue Jun 14 13:00:32 2022
Terminated at Tue Jun 14 13:05:18 2022
Results reported at Tue Jun 14 13:05:18 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python scripts/main.py --cfg configs/baseline.yaml
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   8.96 sec.
    Max Memory :                                 3528 MB
    Average Memory :                             1120.25 MB
    Total Requested Memory :                     12288.00 MB
    Delta Memory :                               8760.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   297 sec.
    Turnaround time :                            302 sec.

The output (if any) follows:

Error processing line 1 of /cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/distutils-precedence.pth:

  Traceback (most recent call last):
    File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site.py", line 168, in addpackage
      exec(line)
    File "<string>", line 1, in <module>
  ModuleNotFoundError: No module named '_distutils_hack'

Remainder of file ignored
2022-06-14 13:00:41.263 | INFO     | __main__:<module>:122 - Input arguments: 
 Namespace(cfg='configs/baseline.yaml', fdr=False)
2022-06-14 13:00:41.322 | INFO     | __main__:main:31 - Hyperparameters: 
 DATASET:
  BATCH_SIZE: 64
  DATASETS_AND_RATIOS: mpii_3dpw_0.5_0.5
  FOCAL_LENGTH: 5000.0
  IGNORE_3D: False
  IMG_RES: 128
  LOAD_TYPE: Base
  MESH_COLOR: pinkish
  NUM_IMAGES: -1
  NUM_WORKERS: 16
  PIN_MEMORY: True
  RENDER_RES: 480
  SHUFFLE_TRAIN: True
  TEST_NUM_IMAGES: -1
  TRAIN_DS: all
  TRAIN_NUM_IMAGES: -1
  VAL_DS: 3dpw-val
EXP_NAME: hmr_baseline
HMR:
  BETA_LOSS_WEIGHT: 0.001
  GT_TRAIN_WEIGHT: 1.0
  KEYPOINT_LOSS_WEIGHT: 5.0
  KEYPOINT_NATIVE_LOSS_WEIGHT: 5.0
  LOSS_WEIGHT: 60.0
  OPENPOSE_TRAIN_WEIGHT: 0.0
  POSE_LOSS_WEIGHT: 1.0
  SHAPE_LOSS_WEIGHT: 0
  SMPL_PART_LOSS_WEIGHT: 1.0
LOG_DIR: logs/baseline
METHOD: baseline
OPTIMIZER:
  LR: 5e-05
  TYPE: adam
  WD: 0.0
PROJECT_NAME: mp2022
RUN_TEST: False
SEED_VALUE: -1
TESTING:
  MULTI_SIDEVIEW: False
  SAVE_FREQ: 1
  SAVE_IMAGES: False
  SAVE_MESHES: False
  SAVE_RESULTS: True
  SIDEVIEW: True
  TEST_ON_TRAIN_END: True
  USE_GT_CAM: False
TRAINING:
  CHECK_VAL_EVERY_N_EPOCH: 1
  LOG_FREQ_TB_IMAGES: 2000
  LOG_SAVE_INTERVAL: 50
  MAX_EPOCHS: 50
  PRETRAINED: None
  PRETRAINED_LIT: None
  RELOAD_DATALOADERS_EVERY_EPOCH: True
  RESUME: None
  SAVE_IMAGES: True
  TEST_BEFORE_TRAINING: False
  USE_AMP: False
2022-06-14 13:00:43.939 | INFO     | hps_core.dataset.base_dataset:__init__:132 - Loaded mpii dataset, num samples 14667
2022-06-14 13:00:44.907 | INFO     | hps_core.dataset.base_dataset:__init__:132 - Loaded 3dpw dataset, num samples 11368
2022-06-14 13:00:44.907 | INFO     | hps_core.dataset.mixed_dataset:__init__:39 - Using ['mpii', '3dpw'] dataset
2022-06-14 13:00:44.907 | INFO     | hps_core.dataset.mixed_dataset:__init__:40 - Ratios of datasets: [0.5, 0.5]
2022-06-14 13:00:45.133 | INFO     | hps_core.dataset.base_dataset:__init__:132 - Loaded 3dpw-val dataset, num samples 3471
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
2022-06-14 13:00:52.030 | INFO     | __main__:main:106 - *** Started training ***
WARNING: You are using a SMPL model, with only 10 shape coefficients.
WARNING: You are using a SMPL model, with only 10 shape coefficients.
WARNING: You are using a SMPL model, with only 10 shape coefficients.
WARNING: You are using a SMPL model, with only 10 shape coefficients.
WARNING: You are using a SMPL model, with only 10 shape coefficients.
WARNING: You are using a SMPL model, with only 10 shape coefficients.
WARNING: You are using a SMPL model, with only 10 shape coefficients.
Traceback (most recent call last):
  File "scripts/main.py", line 125, in <module>
    main(hparams, fast_dev_run=args.fdr)
  File "scripts/main.py", line 107, in main
    trainer.fit(model)
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 510, in fit
    results = self.accelerator_backend.train()
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 57, in train
    return self.train_or_test()
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in train_or_test
    self.trainer.train_loop.setup_training()
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 137, in setup_training
    ref_model.summarize(mode=self.trainer.weights_summary)
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py", line 1379, in summarize
    model_summary = ModelSummary(self, mode=mode)
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/pytorch_lightning/core/memory.py", line 182, in __init__
    self._layer_summary = self.summarize()
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/pytorch_lightning/core/memory.py", line 219, in summarize
    self._forward_example_input()
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/pytorch_lightning/core/memory.py", line 244, in _forward_example_input
    model(input_)
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./hps_core/core/trainer.py", line 113, in forward
    return self.model(x)
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./hps_core/models/dummy_model.py", line 191, in forward
    pred_pose, pred_shape, pred_cam = self.hmr(x)
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./hps_core/models/hmr.py", line 143, in forward
    xf = self.avgpool(x4)
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/aheser/miniconda3/envs/hps-env/lib/python3.7/site-packages/torch/nn/modules/pooling.py", line 599, in forward
    self.padding, self.ceil_mode, self.count_include_pad, self.divisor_override)
RuntimeError: Given input size: (2048x4x4). Calculated output size: (2048x-2x-2). Output size is too small
